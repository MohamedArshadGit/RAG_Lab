{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f79308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "import tiktoken\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory #to save single conversation history\n",
    "from langchain.chains import ConversationalRetrievalChain # which connects llm rag ,he core of RAG, retrieves documents and answers questions conversationally.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6afc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588fcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(root_path:str):\n",
    "    \"\"\"\n",
    "    Loads all markdown(.md) Files from a directory\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        path= root_path,\n",
    "        glob=\"**/*.md\", # go into all folders and fetch .md file \n",
    "        loader_cls=TextLoader, #used textloader for each .md file\n",
    "        loader_kwargs={\n",
    "         \"encoding\":\"utf-8\",\n",
    "         \"autodetect_encoding\":True\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "        docs =loader.load()\n",
    "\n",
    "        print( f\"loaded {len(docs)} documents from directory {root_path}\")\n",
    "        return docs\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'error loading documents {e}')\n",
    "        # return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98545174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 76 documents from directory C:\\Users\\Mohamed Arshad\\Downloads\\My_RAG_Lab\\llm_engineering\\RAG\\knowledge-base\n"
     ]
    }
   ],
   "source": [
    "path =r\"C:\\Users\\Mohamed Arshad\\Downloads\\My_RAG_Lab\\llm_engineering\\RAG\\knowledge-base\"\n",
    "\n",
    "documents =load_documents(root_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df7832f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'C:\\\\Users\\\\Mohamed Arshad\\\\Downloads\\\\My_RAG_Lab\\\\llm_engineering\\\\RAG\\\\knowledge-base\\\\products\\\\Rellm.md'}, page_content=\"# Product Summary\\n\\n# Rellm: AI-Powered Enterprise Reinsurance Solution\\n\\n## Summary\\n\\nRellm is an innovative enterprise reinsurance product developed by Insurellm, designed to transform the way reinsurance companies operate. Harnessing the power of artificial intelligence, Rellm offers an advanced platform that redefines risk management, enhances decision-making processes, and optimizes operational efficiencies within the reinsurance industry. With seamless integrations and robust analytics, Rellm enables insurers to proactively manage their portfolios and respond to market dynamics with agility.\\n\\n## Features\\n\\n### AI-Driven Analytics\\nRellm utilizes cutting-edge AI algorithms to provide predictive insights into risk exposures, enabling users to forecast trends and make informed decisions. Its real-time data analysis empowers reinsurance professionals with actionable intelligence.\\n\\n### Seamless Integrations\\nRellm's architecture is designed for effortless integration with existing systems. Whether it's policy management, claims processing, or financial reporting, Rellm connects seamlessly with diverse data sources to create a unified ecosystem.\\n\\n### Risk Assessment Module\\nThe comprehensive risk assessment module within Rellm allows insurers to evaluate risk profiles accurately. By leveraging historical data and advanced modeling techniques, Rellm provides a clear picture of potential liabilities and expected outcomes.\\n\\n### Customizable Dashboard\\nRellm features a customizable dashboard that presents key metrics and performance indicators in an intuitive interface. Users can tailor their view to focus on what matters most to their business, enhancing user experience and productivity.\\n\\n### Regulatory Compliance Tools\\nRellm includes built-in compliance tracking features to help organizations meet local and international regulatory standards. This ensures that reinsurance practices remain transparent and accountable.\\n\\n### Client and Broker Portals\\nRellm offers dedicated portals for both clients and brokers, facilitating real-time communication and documentation sharing. This strengthens partnerships and drives operational excellence across the board.\\n\\n## Pricing\\n\\nInsurellm offers flexible pricing plans for Rellm to cater to various business needs:\\n\\n- **Basic Plan**: $5,000/month\\n  - Includes access to core features and standard integrations.\\n  \\n- **Professional Plan**: $10,000/month\\n  - Includes all features, advanced integrations, and priority customer support.\\n  \\n- **Enterprise Plan**: Custom pricing\\n  - Tailored solutions with personalized features, extensive integrations, and dedicated account management.\\n\\nJoin the growing number of organizations leveraging Rellm to enhance their reinsurance processes while driving profitability and compliance. \\n\\n## 2025-2026 Roadmap\\n\\nAt Insurellm, we are committed to the continuous improvement of Rellm. Our roadmap for 2025-2026 includes:\\n\\n- **Q3 2025**: \\n  - Launch of the Rellm Mobile App for on-the-go insights and management.\\n  - Introduction of augmented reality (AR) features for interactive risk assessments.\\n\\n- **Q1 2026**: \\n  - Deployment of advanced machine learning models for even more accurate risk predictions.\\n  - Expansion of integration capabilities to support emerging technologies in the insurance sector.\\n\\n- **Q3 2026**: \\n  - Release of a community platform for Rellm users to exchange insights, tips, and best practices.\\n  - Launch of Rellm 2.0, featuring enhanced user interface and premium features based on user feedback.\\n\\nExperience the future of reinsurance with Rellm, where innovation meets reliability. Let Insurellm help you navigate the complexities of the reinsurance market smarter and faster.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4af0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(docs):\n",
    "    \"\"\"\n",
    "    Count the no. of. tokens\n",
    "    \"\"\"\n",
    "    enc =tiktoken.get_encoding(\"cl100k_base\")\n",
    "    total_tokens=0\n",
    "\n",
    "    for d in docs:\n",
    "        num_tokens =len(enc.encode(d.page_content))\n",
    "        total_tokens =total_tokens+num_tokens\n",
    "    \n",
    "    print(f\"total no of tokens are :{total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6841d525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of tokens are :63715\n"
     ]
    }
   ],
   "source": [
    "total =count_tokens(docs=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17eb96b",
   "metadata": {},
   "source": [
    "Divide into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c49e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413\n"
     ]
    }
   ],
   "source": [
    "text_splitter =RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunks =text_splitter.split_documents(documents=documents)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0196ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chunking Guidelines:\n",
    "# -------------------------------\n",
    "# chunk_size: number of tokens per chunk\n",
    "#   - Increase -> Fewer chunks, more context, better for summarization\n",
    "#               Higher embedding cost, less precise retrieval\n",
    "#   - Decrease -> More chunks, less context, better for precise retrieval\n",
    "#               Lower token cost, may break context\n",
    "\n",
    "# chunk_overlap: number of tokens to repeat between chunks\n",
    "#   - Increase -> Preserves context, smoother transitions, higher cost\n",
    "#   - Decrease -> Less redundancy, cheaper, may break context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7959729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key exists\n"
     ]
    }
   ],
   "source": [
    "openai_api_key =os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print('key exists')\n",
    "else:\n",
    "    print('key not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cafa7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings =OpenAIEmbeddings()\n",
    "\n",
    "#Free embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db_name ='vector_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4af1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Chroma database folder exists, wipe its contents before creating a new collection\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name,embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3decbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma VectorStore\n",
    "vector_store =Chroma.from_documents(documents=chunks,embedding=embeddings,persist_directory=db_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ede7ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store has 413 documents\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# check Chroma vector store info\n",
    "data = vector_store._collection.get(include=[\"documents\", \"embeddings\"])\n",
    "num_docs = len(data[\"documents\"])\n",
    "embedding_dim = len(data[\"embeddings\"][0])\n",
    "\n",
    "print(f\"Vector store has {num_docs} documents\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c619093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "llm=ChatOpenAI(model='gpt-4.1-nano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "060a10aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed Arshad\\AppData\\Local\\Temp\\ipykernel_35476\\2071852364.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history',return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# conversation memory for chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history',return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "625cff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever =vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26b44320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting Together\n",
    "conversation_chain =ConversationalRetrievalChain.from_llm(llm=llm,retriever=retriever,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97db87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message,history):\n",
    "    result =conversation_chain.invoke({\"question\":message})\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce211800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed Arshad\\Downloads\\My_RAG_Lab\\llm_engineering\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3d3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
